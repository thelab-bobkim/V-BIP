"""
V-BIP Phase 3-B: Knowledge Base Builder
PDF ë¬¸ì„œì—ì„œ Vector Database êµ¬ì¶•
"""

import os
import json
from typing import List, Dict, Any
from pathlib import Path
import hashlib

try:
    import pdfplumber
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings
    from loguru import logger
except ImportError as e:
    print(f"âš ï¸ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: {e}")
    print("pip install -r requirements_phase3b.txt")

from config_ai import AIConfig


class KnowledgeBaseBuilder:
    """NetBackup ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•"""
    
    def __init__(self):
        self.config = AIConfig
        self.embeddings = None
        self.vectorstore = None
        self.documents = []
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.config.KNOWLEDGE_BASE_DIR, exist_ok=True)
        os.makedirs(self.config.CHROMA_PERSIST_DIR, exist_ok=True)
        
        logger.info("KnowledgeBaseBuilder ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:
        """
        PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            List of documents with metadata
        """
        logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì¤‘: {pdf_path}")
        documents = []
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                logger.info(f"   ì´ {total_pages} í˜ì´ì§€")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    text = page.extract_text()
                    
                    if text and len(text.strip()) > 50:
                        doc = {
                            'content': text,
                            'metadata': {
                                'source': os.path.basename(pdf_path),
                                'page': page_num,
                                'total_pages': total_pages,
                                'doc_type': 'troubleshooting_guide' if 'troubleshooting' in pdf_path.lower() else 'admin_guide'
                            }
                        }
                        documents.append(doc)
                    
                    if page_num % 50 == 0:
                        logger.info(f"   ì§„í–‰: {page_num}/{total_pages} í˜ì´ì§€")
                
                logger.success(f"âœ… {len(documents)} í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
        
        return documents
    
    def chunk_documents(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            documents: ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Chunked documents with metadata
        """
        logger.info("ğŸ“ ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.CHUNK_SIZE,
            chunk_overlap=self.config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        chunked_docs = []
        
        for doc in documents:
            chunks = text_splitter.split_text(doc['content'])
            
            for idx, chunk in enumerate(chunks):
                chunked_doc = {
                    'content': chunk,
                    'metadata': {
                        **doc['metadata'],
                        'chunk_id': idx,
                        'total_chunks': len(chunks),
                        'chunk_hash': hashlib.md5(chunk.encode()).hexdigest()
                    }
                }
                chunked_docs.append(chunked_doc)
        
        logger.success(f"âœ… {len(chunked_docs)} ì²­í¬ ìƒì„± ì™„ë£Œ")
        return chunked_docs
    
    def build_vector_store(self, documents: List[Dict[str, Any]]):
        """
        Vector Store êµ¬ì¶• (ChromaDB)
        
        Args:
            documents: ì²­í¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("ğŸ”¨ Vector Store êµ¬ì¶• ì¤‘...")
        
        try:
            # OpenAI Embeddings ì´ˆê¸°í™”
            self.embeddings = OpenAIEmbeddings(
                model=self.config.EMBEDDING_MODEL,
                openai_api_key=self.config.OPENAI_API_KEY
            )
            
            # ë¬¸ì„œ ì¤€ë¹„
            texts = [doc['content'] for doc in documents]
            metadatas = [doc['metadata'] for doc in documents]
            
            logger.info(f"   ì´ {len(texts)} í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            
            # ChromaDB Vector Store ìƒì„±
            self.vectorstore = Chroma.from_texts(
                texts=texts,
                embedding=self.embeddings,
                metadatas=metadatas,
                collection_name=self.config.CHROMA_COLLECTION_NAME,
                persist_directory=self.config.CHROMA_PERSIST_DIR
            )
            
            logger.success("âœ… Vector Store êµ¬ì¶• ì™„ë£Œ")
            logger.info(f"   ì €ì¥ ìœ„ì¹˜: {self.config.CHROMA_PERSIST_DIR}")
            
        except Exception as e:
            logger.error(f"âŒ Vector Store êµ¬ì¶• ì‹¤íŒ¨: {e}")
            raise
    
    def build_from_pdfs(self, pdf_paths: List[str]):
        """
        PDF íŒŒì¼ë“¤ë¡œë¶€í„° ì „ì²´ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•
        
        Args:
            pdf_paths: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("=" * 60)
        logger.info("ğŸš€ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘")
        logger.info("=" * 60)
        
        all_documents = []
        
        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for pdf_path in pdf_paths:
            if not os.path.exists(pdf_path):
                logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {pdf_path}")
                continue
            
            docs = self.extract_text_from_pdf(pdf_path)
            all_documents.extend(docs)
        
        logger.info(f"ğŸ“Š ì „ì²´ ì¶”ì¶œ: {len(all_documents)} í˜ì´ì§€")
        
        # 2. ë¬¸ì„œ ì²­í‚¹
        chunked_docs = self.chunk_documents(all_documents)
        
        # 3. Vector Store êµ¬ì¶•
        self.build_vector_store(chunked_docs)
        
        # 4. í†µê³„ ì €ì¥
        self.save_statistics(pdf_paths, all_documents, chunked_docs)
        
        logger.info("=" * 60)
        logger.success("âœ… ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        logger.info("=" * 60)
    
    def save_statistics(self, pdf_paths: List[str], documents: List[Dict], chunks: List[Dict]):
        """ì§€ì‹ ë² ì´ìŠ¤ í†µê³„ ì €ì¥"""
        stats = {
            'build_date': str(Path.ctime(Path.cwd())),
            'total_pdfs': len(pdf_paths),
            'pdf_files': [os.path.basename(p) for p in pdf_paths],
            'total_pages': len(documents),
            'total_chunks': len(chunks),
            'avg_chunk_size': sum(len(c['content']) for c in chunks) / len(chunks) if chunks else 0,
            'embedding_model': self.config.EMBEDDING_MODEL,
            'chunk_size': self.config.CHUNK_SIZE,
            'chunk_overlap': self.config.CHUNK_OVERLAP
        }
        
        stats_path = os.path.join(self.config.CHROMA_PERSIST_DIR, 'kb_statistics.json')
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š í†µê³„ ì €ì¥: {stats_path}")
        logger.info(f"   - PDF: {stats['total_pdfs']}ê°œ")
        logger.info(f"   - í˜ì´ì§€: {stats['total_pages']}ê°œ")
        logger.info(f"   - ì²­í¬: {stats['total_chunks']}ê°œ")
        logger.info(f"   - í‰ê·  ì²­í¬ í¬ê¸°: {stats['avg_chunk_size']:.0f} ë¬¸ì")
    
    def test_search(self, query: str, k: int = 3):
        """ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        if not self.vectorstore:
            logger.error("Vector Storeê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'")
        results = self.vectorstore.similarity_search(query, k=k)
        
        for idx, result in enumerate(results, 1):
            logger.info(f"\nê²°ê³¼ {idx}:")
            logger.info(f"  ì¶œì²˜: {result.metadata.get('source', 'Unknown')}")
            logger.info(f"  í˜ì´ì§€: {result.metadata.get('page', 'N/A')}")
            logger.info(f"  ë‚´ìš© (ì• 200ì): {result.page_content[:200]}...")
    
    @classmethod
    def load_existing(cls):
        """ê¸°ì¡´ Vector Store ë¡œë“œ"""
        instance = cls()
        
        if not os.path.exists(AIConfig.CHROMA_PERSIST_DIR):
            logger.warning("ê¸°ì¡´ Vector Storeê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € build_from_pdfs()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        try:
            instance.embeddings = OpenAIEmbeddings(
                model=AIConfig.EMBEDDING_MODEL,
                openai_api_key=AIConfig.OPENAI_API_KEY
            )
            
            instance.vectorstore = Chroma(
                collection_name=AIConfig.CHROMA_COLLECTION_NAME,
                embedding_function=instance.embeddings,
                persist_directory=AIConfig.CHROMA_PERSIST_DIR
            )
            
            logger.success("âœ… ê¸°ì¡´ Vector Store ë¡œë“œ ì™„ë£Œ")
            return instance
            
        except Exception as e:
            logger.error(f"âŒ Vector Store ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # 1. ì„¤ì • ê²€ì¦
    if not AIConfig.validate():
        return
    
    AIConfig.print_config()
    
    # 2. ì§€ì‹ ë² ì´ìŠ¤ ë¹Œë” ì´ˆê¸°í™”
    builder = KnowledgeBaseBuilder()
    
    # 3. PDF íŒŒì¼ ê²½ë¡œ í™•ì¸
    kb_dir = AIConfig.KNOWLEDGE_BASE_DIR
    pdf_files = [
        os.path.join(kb_dir, pdf_name)
        for pdf_name in AIConfig.PDF_DOCS
    ]
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    existing_files = [f for f in pdf_files if os.path.exists(f)]
    
    if not existing_files:
        logger.error(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logger.info(f"ë‹¤ìŒ ìœ„ì¹˜ì— PDFë¥¼ ë°°ì¹˜í•˜ì„¸ìš”: {kb_dir}")
        logger.info(f"í•„ìš”í•œ íŒŒì¼:")
        for pdf in AIConfig.PDF_DOCS:
            logger.info(f"  - {pdf}")
        return
    
    # 4. ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•
    builder.build_from_pdfs(existing_files)
    
    # 5. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    logger.info("\nğŸ§ª ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    test_queries = [
        "Error code 83 media open error",
        "How to restart NetBackup services",
        "Shared memory segment error solution"
    ]
    
    for query in test_queries:
        builder.test_search(query, k=2)
        print()


if __name__ == "__main__":
    main()
