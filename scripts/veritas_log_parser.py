#!/usr/bin/env python3
"""
V-BIP Veritas Log Parser
Veritas NetBackup ë¡œê·¸ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ PostgreSQLì— ì €ì¥
"""

import re
import sys
import argparse
from datetime import datetime
from typing import List, Dict, Optional
import psycopg2
from psycopg2.extras import RealDictCursor

class VeritasLogParser:
    """Veritas NetBackup ë¡œê·¸ íŒŒì„œ"""
    
    # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
    PATTERNS = {
        'timestamp': re.compile(r'(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2}:\d{2})'),
        
        'job_start': re.compile(
            r'(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2}:\d{2})\s+-\s+Info.*'
            r'backup started.*client\s+(\S+),\s+policy\s+(\S+)'
        ),
        
        'error': re.compile(
            r'(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2}:\d{2})\s+-\s+Error.*'
            r'exit status\s*=\s*(\d+)'
        ),
        
        'error_detail': re.compile(
            r'ERR\s+-\s+(.+?)(?:,\s*exit status|$)'
        ),
        
        'job_end': re.compile(
            r'(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2}:\d{2})\s+-\s+Info.*'
            r'backup\s+(completed|failed).*client\s+(\S+)'
        ),
        
        'bytes_written': re.compile(
            r'kilobytes transferred:\s+(\d+)'
        ),
        
        'files_backed_up': re.compile(
            r'files backed up:\s+(\d+)'
        )
    }
    
    def __init__(self, db_config: Dict[str, str]):
        """
        Args:
            db_config: PostgreSQL ì—°ê²° ì„¤ì •
                {host, database, user, password, port}
        """
        self.db_config = db_config
        self.conn = None
        self.cursor = None
        
    def connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.conn = psycopg2.connect(**self.db_config)
            self.cursor = self.conn.cursor(cursor_factory=RealDictCursor)
            print(f"âœ“ PostgreSQL ì—°ê²° ì„±ê³µ: {self.db_config['database']}")
        except Exception as e:
            print(f"âœ— DB ì—°ê²° ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    def close_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()
    
    def parse_timestamp(self, timestamp_str: str) -> datetime:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
        return datetime.strptime(timestamp_str, '%m/%d/%Y %H:%M:%S')
    
    def get_customer_id(self, customer_code: str) -> Optional[int]:
        """ê³ ê°ì‚¬ ì½”ë“œë¡œ ID ì¡°íšŒ"""
        query = "SELECT customer_id FROM customers WHERE customer_code = %s"
        self.cursor.execute(query, (customer_code,))
        result = self.cursor.fetchone()
        return result['customer_id'] if result else None
    
    def parse_log_file(self, file_path: str, customer_code: str, uploader_email: str = 'system') -> Dict:
        """
        ë¡œê·¸ íŒŒì¼ íŒŒì‹±
        
        Args:
            file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
            customer_code: ê³ ê°ì‚¬ ì½”ë“œ (ì˜ˆ: 'SAMSUNG')
            uploader_email: ì—…ë¡œë” ì´ë©”ì¼
            
        Returns:
            íŒŒì‹± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        jobs = []
        current_job = None
        line_number = 0
        
        print(f"\nğŸ“„ ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì‹œì‘: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line_number += 1
                    line = line.strip()
                    
                    if not line:
                        continue
                    
                    # Job ì‹œì‘ ê°ì§€
                    match = self.PATTERNS['job_start'].search(line)
                    if match:
                        if current_job:  # ì´ì „ ì‘ì—… ì €ì¥
                            jobs.append(current_job)
                        
                        current_job = {
                            'customer_code': customer_code,
                            'start_time': self.parse_timestamp(match.group(1)),
                            'server_name': match.group(2),
                            'policy_name': match.group(3),
                            'status': 'Running',
                            'errors': [],
                            'bytes_written': 0,
                            'files_backed_up': 0
                        }
                        continue
                    
                    # ì—ëŸ¬ ê°ì§€
                    match = self.PATTERNS['error'].search(line)
                    if match and current_job:
                        error_code = match.group(2)
                        error_time = self.parse_timestamp(match.group(1))
                        
                        # ì—ëŸ¬ ìƒì„¸ ë©”ì‹œì§€ ì¶”ì¶œ
                        detail_match = self.PATTERNS['error_detail'].search(line)
                        error_message = detail_match.group(1).strip() if detail_match else line
                        
                        current_job['errors'].append({
                            'time': error_time,
                            'code': error_code,
                            'message': error_message
                        })
                        continue
                    
                    # Job ì¢…ë£Œ ê°ì§€
                    match = self.PATTERNS['job_end'].search(line)
                    if match and current_job:
                        current_job['end_time'] = self.parse_timestamp(match.group(1))
                        current_job['status'] = 'Failed' if match.group(2) == 'failed' else 'Success'
                        
                        # Duration ê³„ì‚°
                        if 'end_time' in current_job and 'start_time' in current_job:
                            duration = (current_job['end_time'] - current_job['start_time']).total_seconds() / 60
                            current_job['duration_minutes'] = int(duration)
                        
                        jobs.append(current_job)
                        current_job = None
                        continue
                    
                    # Bytes written ê°ì§€
                    match = self.PATTERNS['bytes_written'].search(line)
                    if match and current_job:
                        kb = int(match.group(1))
                        current_job['bytes_written'] = kb * 1024  # KB to Bytes
                        continue
                    
                    # Files backed up ê°ì§€
                    match = self.PATTERNS['files_backed_up'].search(line)
                    if match and current_job:
                        current_job['files_backed_up'] = int(match.group(1))
                        continue
            
            # ë§ˆì§€ë§‰ ì‘ì—… ì €ì¥
            if current_job:
                jobs.append(current_job)
            
            print(f"âœ“ íŒŒì‹± ì™„ë£Œ: {len(jobs)}ê°œ ì‘ì—… ë°œê²¬")
            
            # í†µê³„
            errors_found = sum(len(job['errors']) for job in jobs)
            print(f"  - ì—ëŸ¬ ë°œê²¬: {errors_found}ê±´")
            
            return {
                'success': True,
                'jobs_count': len(jobs),
                'errors_count': errors_found,
                'jobs': jobs
            }
            
        except FileNotFoundError:
            print(f"âœ— íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return {'success': False, 'error': 'File not found'}
        except Exception as e:
            print(f"âœ— íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def save_to_database(self, parse_result: Dict, customer_code: str, uploader_email: str,
                         log_file_name: str, log_source: str = 'Email') -> bool:
        """
        íŒŒì‹± ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        
        Args:
            parse_result: parse_log_file() ë°˜í™˜ê°’
            customer_code: ê³ ê°ì‚¬ ì½”ë“œ
            uploader_email: ì—…ë¡œë” ì´ë©”ì¼
            log_file_name: ë¡œê·¸ íŒŒì¼ ì´ë¦„
            log_source: ë¡œê·¸ ì†ŒìŠ¤ ('Email', 'WebUpload', 'Agent')
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not parse_result['success']:
            return False
        
        # ê³ ê°ì‚¬ ID ì¡°íšŒ
        customer_id = self.get_customer_id(customer_code)
        if not customer_id:
            print(f"âœ— ê³ ê°ì‚¬ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {customer_code}")
            return False
        
        print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
        
        try:
            jobs = parse_result['jobs']
            saved_count = 0
            
            for job in jobs:
                # ì—ëŸ¬ ì •ë³´ ì¶”ì¶œ
                error_code = None
                error_message = None
                exit_code = 0
                
                if job['errors']:
                    first_error = job['errors'][0]
                    error_code = first_error['code']
                    error_message = first_error['message']
                    exit_code = int(error_code)
                
                # Backup type ì¶”ì • (policy ì´ë¦„ì—ì„œ)
                backup_type = 'Incremental'
                if 'FULL' in job['policy_name'].upper():
                    backup_type = 'Full'
                elif 'DIFF' in job['policy_name'].upper():
                    backup_type = 'Differential'
                
                # INSERT ì¿¼ë¦¬
                insert_query = """
                INSERT INTO backup_jobs 
                (customer_id, server_name, policy_name, job_name, backup_type,
                 start_time, end_time, duration_minutes, status, exit_code, 
                 error_code, error_message, bytes_written, files_backed_up,
                 log_source, uploaded_by, raw_log_file_path)
                VALUES 
                (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING job_id
                """
                
                values = (
                    customer_id,
                    job['server_name'],
                    job['policy_name'],
                    f"{customer_code}_{job['server_name']}_{backup_type}",
                    backup_type,
                    job['start_time'],
                    job.get('end_time'),
                    job.get('duration_minutes'),
                    job['status'],
                    exit_code,
                    error_code,
                    error_message,
                    job['bytes_written'],
                    job['files_backed_up'],
                    log_source,
                    uploader_email,
                    log_file_name
                )
                
                self.cursor.execute(insert_query, values)
                job_id = self.cursor.fetchone()['job_id']
                saved_count += 1
                
                # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì•Œë¦¼ íˆìŠ¤í† ë¦¬ ìƒì„± (ë‚˜ì¤‘ì— n8nì´ ì²˜ë¦¬)
                if error_code:
                    print(f"  âš ï¸  Job {job_id}: {job['server_name']} - Error {error_code}")
            
            # ë¡œê·¸ ì—…ë¡œë“œ íˆìŠ¤í† ë¦¬ ì €ì¥
            upload_query = """
            INSERT INTO log_uploads
            (customer_id, upload_method, file_name, uploader_email, 
             parsing_status, jobs_extracted, errors_found)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            """
            
            self.cursor.execute(upload_query, (
                customer_id,
                log_source,
                log_file_name,
                uploader_email,
                'Success',
                len(jobs),
                parse_result['errors_count']
            ))
            
            self.conn.commit()
            
            print(f"âœ“ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì‘ì—…")
            return True
            
        except Exception as e:
            self.conn.rollback()
            print(f"âœ— DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False


def main():
    parser = argparse.ArgumentParser(description='Veritas NetBackup ë¡œê·¸ íŒŒì„œ')
    parser.add_argument('--file', required=True, help='ë¡œê·¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--customer', required=True, help='ê³ ê°ì‚¬ ì½”ë“œ (ì˜ˆ: SAMSUNG)')
    parser.add_argument('--uploader', default='system', help='ì—…ë¡œë” ì´ë©”ì¼')
    parser.add_argument('--source', default='Email', choices=['Email', 'WebUpload', 'Agent'],
                       help='ë¡œê·¸ ì†ŒìŠ¤')
    parser.add_argument('--db-host', default='localhost', help='PostgreSQL í˜¸ìŠ¤íŠ¸')
    parser.add_argument('--db-port', default='5432', help='PostgreSQL í¬íŠ¸')
    parser.add_argument('--db-name', default='veritas_monitor', help='ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„')
    parser.add_argument('--db-user', default='postgres', help='DB ì‚¬ìš©ì')
    parser.add_argument('--db-password', default='', help='DB ë¹„ë°€ë²ˆí˜¸')
    
    args = parser.parse_args()
    
    # DB ì„¤ì •
    db_config = {
        'host': args.db_host,
        'port': args.db_port,
        'database': args.db_name,
        'user': args.db_user,
        'password': args.db_password
    }
    
    # íŒŒì„œ ì´ˆê¸°í™”
    parser_instance = VeritasLogParser(db_config)
    parser_instance.connect_db()
    
    # ë¡œê·¸ íŒŒì‹±
    result = parser_instance.parse_log_file(args.file, args.customer, args.uploader)
    
    # DB ì €ì¥
    if result['success']:
        import os
        log_file_name = os.path.basename(args.file)
        success = parser_instance.save_to_database(
            result, args.customer, args.uploader, log_file_name, args.source
        )
        
        if success:
            print(f"\nâœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
            print(f"   ì‘ì—…: {result['jobs_count']}ê±´")
            print(f"   ì—ëŸ¬: {result['errors_count']}ê±´")
    else:
        print(f"\nâŒ íŒŒì‹± ì‹¤íŒ¨")
        sys.exit(1)
    
    parser_instance.close_db()


if __name__ == '__main__':
    main()
